---
title: "Example of Bayesian Analysis"
date: '`r format(Sys.time(), "%Y-%m-%d")`'
author: "Carl Schwarz"

output:
  pdf_document:
    toc: yes
    number_sections: yes
  html_document:
    number_sections: yes
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Fit a bayesian model to estimate p(being in each risk category) and credible intervals
# without any trend in the study for a single year and a single species
# This uses simulated data to illustrate the impact of changes in the mean


# Open the libraries used in this analysis
library(ggplot2)
library(coda)
library(pander)
library(plyr)
library(R2jags)  # used for call to JAGS
library(reshape2)

# define table and figure numbering
# Keep track of figure and table numbers. Do not modify the global variables .FIGNUM and .TABNUM
getFIGNUM <- function(){
    .FIGNUM <<- .FIGNUM +1
    .FIGNUM
}

getTABNUM <- function(){
  .TABNUM <<- .TABNUM +1
  .TABNUM
}

.FIGNUM <- 0
.TABNUM <- 0




# Get the FSI category boundaries
# Read the FSI Thresholds
FSI.threshold.csv<- textConnection(
"Species.Code, FSI.num.cat, FSI.cat, lower, upper
BKTR, 1, VHR,   0,   35
BKTR, 2,  HR,  35,  90
BKTR, 3,  MR,  90, 120
BKTR, 4,  LR, 120, 170
BKTR, 5, VLR, 170, 3000")

FSI.threshold <- read.csv(FSI.threshold.csv, header=TRUE, as.is=TRUE, strip.white=TRUE)
FSI.cat.unique <- unique(FSI.threshold[,c("FSI.num.cat","FSI.cat")])
FSI.threshold$FSI.cat <- factor( FSI.threshold$FSI.cat, 
                        levels= FSI.cat.unique$FSI.cat[order(FSI.cat.unique$FSI.num.cat)], order=TRUE)


# Make the simulated data
set.seed(23434543)
stream1.cpue <- data.frame(Stream="Stream 1",
                           cpue = round(rlnorm(10, meanlog=log(35), sdlog=.3 ),1))
stream2.cpue <- data.frame(Stream="Stream 2",
                           cpue = round(rlnorm(10, meanlog=log(35), sdlog= .7 ),1))
cpue <- rbind(stream1.cpue, stream2.cpue)

med.cpue <- plyr::ddply(cpue, "Stream", plyr::summarize,
                      med.cpue=median(cpue),
                      med.cpue.lcl=exp( mean(log(cpue))-1.96*sd(log(cpue))/sqrt(length(cpue))),
                      med.cpue.ucl=exp( mean(log(cpue))+1.96*sd(log(cpue))/sqrt(length(cpue))))

```

# Introduction
```{r fsi.tabnum, echo=FALSE}
fsi.tabnum <- getTABNUM()
```
The Fish Sustainability Index (FSI) is Alberta Fish and Wildlife’s 
method of assessing fish stocks on a provincial scale. One of the 
components of this index is Population Integrity which is partially 
assessed using population density. Density comparisons are referenced 
to what the watershed in question could produce if it had no human impacts 
and consisted of the most ideal habitat in Alberta.  The observed density is ranked on six 
point scale (0 to 5) from Functionally Extirpated to Low Risk. 
For example, Table `r fsi.tabnum` presents draft guidelines for BKTR 
based on observed catch per unit effort from one pass electrofishing.
```{r fsi.threshold, echo=FALSE, results='asis'}
table <- FSI.threshold[,c(1,3,4,5)]
colnames(table) <- c('Species\nCode',
                     'FSI\nCategory',
                     'Lower\nbound',
                     'Upper\nbound')
pandoc.table(table,
             caption=paste('Table ',fsi.tabnum,'. FSI categories for BKTR'),
             justify='lrrr',
             split.cells=c(.5,1,1,1))
```
```{r qc.fignum,echo=FALSE}
qc.fignum <- getFIGNUM()
```

Quantitative classification of a given population occurs by sampling a watershed at numerous sites (reaches) and observing the CPUE. However data can be very noisy. Consider, for example, a plot of CPUE for three species at Quirk Creek in Figure `r qc.fignum`.
```{r, out.height="5in",echo=FALSE, include=FALSE, message=FALSE, warning=FALSE, fig.align='center',dpi=300}

# This comes from the Ghost_Quirk_compiledCPUE_AllSpp.xlsx file
Quirk.csv <- textConnection(
"Watershed   ,  HUC   ,  Waterbody Name   ,  Year   ,  Site No   ,  UTM11U_E   ,  UTM11U_N   ,  BKTR   ,  CTTR   ,  BLTR
Quirk   ,  12   ,  Quirk   ,  1995   ,  LW   ,  657209   ,  5631652   ,  2.60   ,  0.40   ,  0.00
Quirk   ,  12   ,  Quirk   ,  1996   ,  LW   ,  657209   ,  5631652   ,  7.80   ,  1.00   ,  0.00
Quirk   ,  12   ,  Quirk   ,  1997   ,  LW   ,  657209   ,  5631652   ,  28.00   ,  8.60   ,  0.60
Quirk   ,  12   ,  Quirk   ,  1998   ,  LW   ,  657209   ,  5631652   ,  30.40   ,  6.00   ,  0.40
Quirk   ,  12   ,  Quirk   ,  1998   ,  UP   ,  660029   ,  5630056   ,  43.16   ,  6.32   ,  1.58
Quirk   ,  12   ,  Quirk   ,  1999   ,  LW   ,  657209   ,  5631652   ,  20.60   ,  3.20   ,  0.00
Quirk   ,  12   ,  Quirk   ,  1999   ,  UP   ,  660029   ,  5630056   ,  31.32   ,  4.74   ,  1.05
Quirk   ,  12   ,  Quirk   ,  2000   ,  LW   ,  657209   ,  5631652   ,  40.00   ,  7.60   ,  0.60
Quirk   ,  12   ,  Quirk   ,  2000   ,  UP   ,  660029   ,  5630056   ,  58.68   ,  15.26   ,  1.05
Quirk   ,  12   ,  Quirk   ,  2002   ,  LW   ,  657209   ,  5631652   ,  21.20   ,  2.20   ,  0.40
Quirk   ,  12   ,  Quirk   ,  2002   ,  UP   ,  660029   ,  5630056   ,  17.11   ,  1.58   ,  0.00
Quirk   ,  12   ,  Quirk   ,  2003   ,  LW   ,  657209   ,  5631652   ,  8.80   ,  16.20   ,  1.40
Quirk   ,  12   ,  Quirk   ,  2003   ,  UP   ,  660029   ,  5630056   ,  18.95   ,  5.00   ,  2.11
Quirk   ,  12   ,  Quirk   ,  2004   ,  LW   ,  657209   ,  5631652   ,  7.00   ,  7.40   ,  1.60
Quirk   ,  12   ,  Quirk   ,  2004   ,  UP   ,  660029   ,  5630056   ,  21.32   ,  11.32   ,  0.53
Quirk   ,  12   ,  Quirk   ,  2005   ,  LW   ,  657209   ,  5631652   ,  19.20   ,  8.60   ,  1.20
Quirk   ,  12   ,  Quirk   ,  2005   ,  UP   ,  660029   ,  5630056   ,  20.26   ,  12.37   ,  2.37
Quirk   ,  12   ,  Quirk   ,  2006   ,  LW   ,  657209   ,  5631652   ,  9.20   ,  2.00   ,  0.80
Quirk   ,  12   ,  Quirk   ,  2006   ,  UP   ,  660029   ,  5630056   ,  22.63   ,  6.84   ,  1.05
Quirk   ,  12   ,  Quirk   ,  2007   ,  LW   ,  657209   ,  5631652   ,  15.60   ,  10.20   ,  2.00
Quirk   ,  12   ,  Quirk   ,  2007   ,  UP   ,  660029   ,  5630056   ,  18.42   ,  18.16   ,  2.11
Quirk   ,  12   ,  Quirk   ,  2008   ,  LW   ,  657209   ,  5631652   ,  2.40   ,  6.40   ,  0.00
Quirk   ,  12   ,  Quirk   ,  2008   ,  UP   ,  660029   ,  5630056   ,  15.00   ,  12.37   ,  0.26
Quirk   ,  12   ,  Quirk   ,  2009   ,  LW   ,  657209   ,  5631652   ,  4.40   ,  7.00   ,  0.00
Quirk   ,  12   ,  Quirk   ,  2009   ,  UP   ,  660029   ,  5630056   ,  5.00   ,  8.42   ,  0.00
Quirk   ,  12   ,  Quirk   ,  2010   ,  LW   ,  657209   ,  5631652   ,  13.80   ,  6.80   ,  0.00
Quirk   ,  12   ,  Quirk   ,  2010   ,  UP   ,  660029   ,  5630056   ,  11.58   ,  10.79   ,  0.53
Quirk   ,  12   ,  Quirk   ,  2011   ,  LW   ,  657209   ,  5631652   ,  14.40   ,  13.60   ,  0.20
Quirk   ,  12   ,  Quirk   ,  2011   ,  UP   ,  660029   ,  5630056   ,  26.84   ,  22.89   ,  0.53
Quirk   ,  12   ,  Quirk   ,  2012   ,  LW   ,  657209   ,  5631652   ,  6.80   ,  6.20   ,  0.80
Quirk   ,  12   ,  Quirk   ,  2012   ,  UP   ,  660029   ,  5630056   ,  9.21   ,  5.53   ,  0.53
Quirk   ,  12   ,  Quirk   ,  2013   ,  LW   ,  657209   ,  5631652   ,  4.20   ,  3.80   ,  0.80
Quirk   ,  12   ,  Quirk   ,  2013   ,  UP   ,  660029   ,  5630056   ,  8.16   ,  4.74   ,  0.26
Quirk   ,  12   ,  Quirk   ,  2014   ,  LW   ,  657209   ,  5631652   ,  3.40   ,  3.40   ,  0.00
Quirk   ,  12   ,  Quirk   ,  2014   ,  UP   ,  660029   ,  5630056   ,  10.26   ,  2.11   ,  0.26")

pass <- read.csv(Quirk.csv, header=TRUE, as.is=TRUE, strip.white=TRUE)

pass.melt <- reshape2::melt(pass,
                            id.vars=c("Watershed","Year","Site.No"),
                            measure.vars=c("BKTR","CTTR","BLTR"),
                            variable.name="Species",
                            value.name="Density")
pass.melt$Species <- as.character(pass.melt$Species)
str(pass.melt)
# note that the data frame has density as fish/100m, but the standards are in 
# terms of fish/300m so we need to multiply by 3
pass.melt$Density <- pass.melt$Density*3


# add in all combinations of year and species so that plot "breaks" when data stops
all.year.species <- expand.grid(Year=min(pass.melt$Year,na.rm=TRUE):max(pass.melt$Year, na.rm=TRUE),
                                Species=unique(pass.melt$Species), 
                                Site.No=unique(pass.melt$Site.No), 
                                Watershed=unique(pass.melt$Watershed),stringsAsFactors=FALSE)
dim(pass.melt)
pass.melt <- merge(pass.melt, all.year.species, all=TRUE)
dim(pass.melt)
xtabs(~Species+Year, data=pass.melt, exclude=NULL, na.action=na.pass)


raw.trend <- ggplot2::ggplot(data=pass.melt, aes(x=Year, y=Density, color=Species, linetype=Site.No))+
   ggtitle(paste("Figure ",qc.fignum,". Raw data for Quirk Creek with FSI categories",sep=""))+
   geom_point()+
   geom_line()+
   geom_hline(data=FSI.threshold, aes(yintercept=lower), alpha=0.2)+
   ylab("Density (fish/300 m2)")
#raw.trend


raw.trend.log <- ggplot2::ggplot(data=pass.melt, aes(x=Year, y=log(Density+.1), color=Species, linetype=Site.No))+
   ggtitle("Raw data for Quirk Creek with FSI categories")+
   geom_point()+
   geom_line()+
   geom_hline(data=FSI.threshold, aes(yintercept=log(lower+.1)), alpha=0.2)+
   ylab("log of Density (fish/300 m2)")
#raw.trend.log
```
```{r echo=FALSE, warning=FALSE, message=FALSE}
raw.trend
```

In some situations, classification is straightforward. For example, if the (very) simplistic assumption is made that the FSI categories are identical for all species, then most readers would clearly place the BLTR (green lines) in the VHR category in all years. However, what should be done with CTTR. In some years, some sites are above the threshold between VHR and HR, while in other years, both values are below the threshold. By making a single classification solely based on the mean or median ignores the variability in the data
and the uncertainty in these estimates.

```{r mb.fignum, echo=FALSE}
mb.fignum <- getFIGNUM()
```
Similarly, consider Figure `r mb.fignum` extracted from results in the Macleod and Berland Watershed. While the mean CPUE all fall within the HR FSI category, the uncertainty in the mean CPUE is large enough that it may actually fall in any of the three categories. 
```{r  out.height="5in",echo=FALSE, fig.align='center'}
knitr::include_graphics(file.path("Figures","mb-cat.png"), dpi=200)
```

Rather than trying to arbitrarily choose a single FSI category 
and ignore uncertainty, it would be useful to view the 
assignment as a probabilistic task. For example, 
we would like to make statements such as “We are 80% certain that 
this watershed is in the HR category”. 

This gives rise to a number of issues that need to be resolved:

(a) How can such an assessment be done?

(b) How can these assessments be tracked over time?

(c) How many sites must be measured so that you are xx% certain that each watershed is in its appropriate risk category?



# Principles of the analysis
## Using MEDIANS rather than MEANS
```{r sd.fignum, echo=FALSE}
sd.fignum <- getFIGNUM()
```
Plots of the standard deviation (between the sites in a year) vs. the 
mean of the sites in a year generally shows a pattern where the 
standard deviation increases with the mean such as seen in Figure `r sd.fignum`
for Quirk Creek:
```{r SD.vs.mean, echo=FALSE, message=FALSE, warning=FALSE, dpi=300, fig.height=4, fig.width=4, out.height="4in"}
sd.vs.mean <- plyr::ddply(pass.melt, c("Watershed","Year","Species"), plyr::summarize,
                          mean.density = mean(Density),
                          sd.density   = sd  (Density))
sdplot <- ggplot2::ggplot(data=sd.vs.mean, aes(x=mean.density, y=sd.density))+
   ggtitle(paste("Figure ", sd.fignum,". SD vs. Mean for Density ",sep=""))+
   xlab("Mean Density (fish / 300 m")+ylab("SD Density (fish / 300 m")+
   geom_point()+
   facet_wrap(~Species, ncol=2, scale="free")
sdplot
```

This is quite common when measuring abundance and 
indicates that a log-normal distribution will be 
good description of the distribution of measured densities
among sites within a year. One of the properties of the log-normal distribution is that large outliers are quite common. To account for the impact of 
these large outliers on the mean across the sites in a year, 
the MEDIAN is preferred measure of overall trend and of classification to the FSI category. The median is the point where 50% of the values (measurements at sites) 
are predicted to be above and 50% of the values are predicted to be below. 

If the underlying distribution of readings follows a log-normal distribution, 
then the median can be estimated by the geometric mean of the raw observations.


## Probabilistic assignment of categories
```{r echo=FALSE}
stream.fignum <- getFIGNUM()
```
Consider Figure `r stream.fignum` of two (simulated) CPUE data sets for two different streams (each sampled for a single year) along with the FSI category boundaries. The "X" indicates the median of the dataset 
and the error bars show 95% confidence intervals for the median of the data.

```{r baseplot, echo=FALSE, fig.height=3, fig.width=5, out.height="4in", dpi=300}
initplot <- ggplot2::ggplot( data=cpue, aes(x=Stream, y=cpue))+
  ggtitle(paste("Figure ", stream.fignum,". Initial plot of CPUE for two streams",sep=""))+
  ylab("CPUE (fish / 300 m)")+
  geom_point( position=position_jitter(w=.1))+
  geom_point(data=med.cpue, aes(y=med.cpue), shape="X", color="blue", size=4)+
  geom_errorbar(data=med.cpue, aes(ymin=med.cpue.lcl, ymax=med.cpue.ucl), width=.05)+
  geom_hline(data=FSI.threshold[1:3,], aes(yintercept=lower), alpha=1, color="red")+
  geom_text( data=FSI.threshold[1:3,], aes(y=lower, x=-Inf), 
             label=FSI.threshold[1:3,]$FSI.cat,
             hjust="left", vjust="bottom", color="red")

initplot
```

We would like to make statements about the category (VHR, HR or MR) to which each stream belongs.
In both streams, the median CPUE (blue X) are in the VHR category, but it does not seem
sensible to definatively rank both streams in the VHR category just because their respective
medians fall in this category. Indeed, the data for Stream 2 is more variable which results
in the 95% confidence interval for the median for Stream 2 being wider than the corresponding
interval for Stream 1. In some sense, we should have a stronger belief that the median CPUE
for Stream 1 is in the VHR category that for Stream 2, but there is some belief that each
stream could also be in HR category.

It seems natural to ask – what is the “probability that the median density is in each category”. 
This cannot be answered with classical statistics (mean and confidence intervals) 
because the actual category membership is fixed (non probabilistic) in any one year. 
A Bayesian analysis comes to the rescue. A Bayesian would change the question 
slightly to “what is the belief that the median density is in each category”. 
The change is crucial, because belief is naturally expressed in a probabilistic framework.

An intuitive explanation for the process is as follows. 
For each year, estimate the parameters that describe that year’s 
distribution of density across the sites. If a log-normal distribution 
is assumed, the parameters desribe the distribution are the log(median density) and the 
standard deviation on the log-scale across ALL possible sites. 
These values are estimated based (on this case) data from each stream. 
The  sampling distribution for the log(median) is estimated. 
A sampling distribution gives the distribution of plausible values 
for the log(median) for each stream. 
[A confidence interval can be computed from this sampling distribution but is not used.]. 
A Bayesian then says that the sampling distribution is interpreted as your 
belief in the distribution of the log(median) for all sites. 
Consequently, compute what fraction of the sampling distribution lies between the FSI boundaries and that is your belief (probability) that the median density is in each category.

The actually fitting process cannot be done by hand and 
requires a method called MCMC to do the Bayesian analysis. 
As part of the output from an MCMC analysis, 
quantities such as the probability (belief) that the median is in each category are easily found.

A common language to describe Bayesian models is  BUGS. There are several computer
programs (WinBugs, OpenBugs, JAGS, etc.) 
that take the Bayesian model described by BUGS and perform a Monte Carlo Markov
Chain (MCMC) analysis to estimate the posterior distributon of the parameters
of interest. We use JAGS which is called 
from R (an open source statistical software package). 

For example, here is the BUGS code to do a probabilistic assignment of each stream
to the FSI category above:

```{r SimpleBayes, echo=TRUE}
cat(file="model1.txt", "
    ############################################################
    # Input data is
    #   Ndata   - number of data points measured for the stream
    #   Density   - the Density as measured by the CPUE at each site

    #   NFSI      - number of FSI categories
    #   FSI.lower - the upper and lower bounds of the FSI
    #   FSI.upper   categories
    ############################################################

  model {
    
    # likelihood - log normal distribution of cpue density values
    for(i in 1:Ndata){
       Density[i] ~ dlnorm(log.median, tau)
    }

    # prior distribution for log.median and tau
    # tau is 1/sd
    tau <- 1/(sd.log*sd.log)
    sd.log ~ dunif(.05, 3)   # on the log-scale sd is proportion of the mean

    # priors for the log.median
    log.median ~dnorm( 0, .00001) # virtually no information in prior

   # derived variables.
   # median.den is antilog of  log.medianl 
   median <- exp(log.median)

   # probability of median being in each threshold category 
   for(k in 1:NFSI){
      prob.FSI.cat[k] <- ifelse((median >= FSI.lower[k]) && (median < FSI.upper[k]),1,0)
   }
}
") # End of the model

```

All Bayesian model have two components.

First, is the likelihood, i.e. what is the probability distribution of the data?
In this case, we are assuming a log-normal distribution (`dlnorm`). Note that in the
BUGS langauge, the second parmaeter (denoted as `tau`) represents 1/variance. The two parameters
that describe this log-normal distribution are the log(median) (`log.median`) and the
standard deviation on the log-scale (`SD.log`). The `for` loop assumes that all of the 
data from this stream comes from the same log-normal distribution.

Second is the prior distribution for the unknown parameters. Here we chose relatively
uninformative priors for both parameters.

During each interation in the MCMC process, the estimates of the parameters (the
`log.median` and `sd.log`) are updated using standard Bayesian methods. This updating
process generates a sample from the posterior distribution as shown later.

We can also compute derived variables for each interation of the MCMC process.
In this case, we take the anti-logarithm of the `log.median` and then use
the estimated median to see in which FSI category it lies. For each iteration, if
the estimated median lies between the lower and upper bound of the FSI category,
it generates a 1 otherwise a 0.  This generated string of 0's and 1's is used
to estimate the probability of falling in each FSI category as explained later.

We now set up _R_ variables to hold the data being passed to JAGS (the data.list),
the initial values for the MCMC chains (we leave these blank and let JAGS generate them),
and which variable we want posteriors to be estimated (the monitor.list).

```{r run.sample1, echo=TRUE, comment=NA}
data.list <- list(Ndata      =length(stream1.cpue$cpue),
                  Density    =stream1.cpue$cpue,  
                  NFSI       =nrow(FSI.threshold),
                  FSI.lower  =FSI.threshold$lower,
                  FSI.upper  =FSI.threshold$upper)
data.list


# Next create the initial values.

init.list <- list(
   list(), list(), list()
)

# Next create the list of parameters to monitor to get posterior
# 
monitor.list <- c("log.median", "sd.log","median","prob.FSI.cat")
```

Now we call JAGS using the `jags()` function from the `R2JAGS` package:
```{r run.jags.stream1, echo=TRUE}

# Finally, the actual call to JAGS
set.seed(4534534)  # intitalize seed for MCMC 

results <- jags( 
  data      =data.list,   # list of data variables
  inits     =init.list,   # list/function for initial values
  parameters=monitor.list,# list of parameters to monitor
  model.file="model1.txt",  # file with bugs model
  n.chains=3,
  n.iter  =5000,          # total iterations INCLUDING burn in
  n.burnin=2000,          # number of burning iterations
  n.thin=2,               # how much to thin
  DIC=TRUE,               # is DIC to be computed?
  working.dir=getwd()    # store results in current working directory
)

```

The output from the call to JAGS (the `results` object) is a complex object
that has many components. We will look at some of them:

There are several thousand samples generated for each posterior. 
Here are some of the samples from the posterior distributions
(extracted from `results$BUGSoutput$sim.matrix`) for the `log.median` and the `median`.
```{r stream1.post.samples, echo=FALSE, comment=NA}
round(results$BUGSoutput$sims.matrix[1:5,c("log.median","median")],2)
```
For each iteration in the MCMC process, a value for the `log.median` and the `median` 
is generated as shown above. These values form the posterior sample for their
respective parameters. 

Then for each of the posterior samples from the median, a 0/1 indicator variable
is created for each FSI category:
```{r stream1.post.samples.fsi, echo=FALSE, comment=NA}
 options(width=200)
    results$BUGSoutput$sims.matrix[1:5,colnames(results$BUGSoutput$sims.matrix)
                                    [grepl("prob.FSI",colnames(results$BUGSoutput$sims.matrix))]]
```
In this case, 4/5 of the posterior samples for the median are in the first
FSI category, and 1/5 of the posterior samples for the median are in the second
fsi category. This would correspond to a 80% posterior belief that the median 
is in the first FSI category and a 20% posterior belief that the median
is in the second FSI category. 

Of course, we look at the averages of the entire set of posterior samples (a grand
total of `r nrow(results$BUGSoutput$sims.matrix)` samples in each posterior sample).
The mean and standard deviation for the `log.median` and `median` are:
```{r stream1.mean.post1, echo=FALSE, comment=NA}
round(results$BUGSoutput$summary[c("log.median","median"),c("mean","sd","2.5%","97.5%")],2)
```
The estmated median of the CPUE for stream 1 is `r round(results$BUGSoutput$mean$median,2)`
with a 95% credible interval of (
`r paste(round(results$BUGSoutput$summary[c("median"),c("2.5%","97.5%")],2),collapse=" - ")`).

Finally, the posterior belief that the median is in each FSI category is
```{r stream1.mean.post2, echo=FALSE, comment=NA}
temp1 <- round(results$BUGSoutput$summary[rownames(results$BUGSoutput$summary)
                                    [grepl("prob.FSI",rownames(results$BUGSoutput$summary))]
                                 ,c("mean"),drop=FALSE],2)
colnames(temp1) <- "Probability"
cat("Probability of Stream 1 in each FSI category")
temp1
```


```{r fit.stream2, echo=FALSE,include=FALSE, comment=NA}


data.list <- list(Ndata      =length(stream2.cpue$cpue),
                  Density    =stream2.cpue$cpue,  
                  NFSI       =nrow(FSI.threshold),
                  FSI.lower  =FSI.threshold$lower,
                  FSI.upper  =FSI.threshold$upper)

results2 <- jags( 
  data      =data.list,   # list of data variables
  inits     =init.list,   # list/function for initial values
  parameters=monitor.list,# list of parameters to monitor
  model.file="model1.txt",  # file with bugs model
  n.chains=3,
  n.iter  =5000,          # total iterations INCLUDING burn in
  n.burnin=2000,          # number of burning iterations
  n.thin=2,               # how much to thin
  DIC=TRUE,               # is DIC to be computed?
  working.dir=getwd()    # store results in current working directory
)
```
The same set of computation can be done for Stream 2 and we obtain the following
results for Stream 2.
```{r stream2.mean.post2, echo=FALSE, comment=NA}
temp2 <- round(results2$BUGSoutput$summary[rownames(results2$BUGSoutput$summary)
                                    [grepl("prob.FSI",rownames(results2$BUGSoutput$summary))]
                                 ,c("mean"),drop=FALSE],2)
colnames(temp2) <- "Probability"
cat("Probability of Stream 2 in each FSI category")
temp2
```

```{r echo=FALSE}
both.fignum <- getFIGNUM()
```
As expected, we see a shift in the posterior beliefs for Stream 2 vs Stream 1. The probabilistic 
assignments can be graphed using the cumulative probability of being in each category as shown
in FIgure `r both.fignum`.

```{r fsi.cat.both, echo=FALSE, fig.height=3, fig.width=5, dpi=300}
# stack the two sets of estimated probabilitities
temp1b <- data.frame(prob=as.vector(temp1),
                    FSI.num.cat=1:5,
                    Stream='Stream 1',
                    stringsAsFactors=FALSE)
temp2b <- data.frame(prob=as.vector(temp2),
                    FSI.num.cat=1:5,
                    Stream='Stream 2',
                    stringsAsFactors=FALSE)
plotdata <- rbind(temp1b, temp2b)
plotdata <- merge(plotdata, FSI.threshold[,c("FSI.num.cat","FSI.cat")])


# we need to sort the data frame in reverse order of FSI category
plotdata <- plotdata[ order(plotdata$FSI.cat),] #, decreasing=TRUE),]
plotdata$FSI.cat2 <- factor(plotdata$FSI.cat, levels=rev(levels(plotdata$FSI.cat)), order=TRUE)
fsi.plot <- ggplot(data=plotdata, aes(x=Stream, y=prob, fill=FSI.cat2))+
  ggtitle(paste("Figure ", both.fignum,". Probability of being in FSI category",sep=""))+
  ylab("Cumulative probability of being in FSI category")+
  geom_bar(stat="identity")+
  scale_fill_brewer(palette="RdYlGn", direction =-1, name="FSI\nCategory")
fsi.plot


```


# Multiple years of data
The previous section showed how a probabilistic assessment could be
made for a single year of data for a watershed. Naively, you could do a similar
assessment for each year. However, there are two problem with such a naive analysis.

(a) Some sites are repeatedly measured over time. We saw in the Quirk Creek examples
above that the same two sites were repeatedly measured over time. This implies
that the data values across years are not independent from each other. For 
example, one site (due to local site-specific effects) could tend to always 
have a higher than average density across years. Any analysis must account
for these repeated measurements over tme

(b) There may be considerable year-specific effects (process error). Year-specific
effects tend to push the densities in all sites up or down in a particular year
due to effects typically that cannot be controlled. For example, a specific year
could be warmer than usual leading to warmer water temperatures and affecting
the efficiency of the electrofishing in all sites in a year. These year-specific effects
could cause the probabilistic assessment to vary considerably from year to year 
due to random events that are unrelated to the long-term trends in the data.
The related document on the number of years and sites needed to detect certain trends
has a fuller discussion of year-specific effects (process error).

Statistical models can be developed to fit trend over time and deal with the above two problems.
Such models are linear mixed models (LMM) and take the form (in standard  _R_ syntax):
`log(Density) ~ Year + YearC(R) + Site(R)`
where the `log(Density)` is the logarithm of the observed density from electrofishing,
`Year` represents the linear trend over time in the log(median); 
`YearC(R)` represents the (random) year-specific effects;
and `Site(R)` represents the (random) site-specific effects.

Both the year-specific and site-specific effects are assumed to follow a Normal 
distribution with mean 0 and year-specific and site-specific standard deviations.

A similar Bayesian model can be developed that accounts for year-specific and site-specific
effects.

```{r trend.model}
cat(file="model2.txt", "
    ############################################################
    # Input data
    #      Ndata  - number of data points
    #      Density- density for each data point
    #      Site.num- site number for each data point (1....NSites)
    #      Year.num- year number for each data point (1... Nyears)

    ############################################################

    # compute the number of years (1...) and number of sites
data {
       Nyears   <- max(Year.num)
       Nsites   <- max(Site.num)
     }    

model {
    
    # compute the trend line
    for(i in 1:Nyears){
          log.median.trend[i] <- beta0 + beta1*i
    }

    # add the site-effects and year-effects to the trend
    for(i in 1:Ndata){
       log.median.data[i] <- log.median.trend[Year.num[i]] + 
                     site.eff[Site.num[i]] +  
                     year.eff[Year.num[i]]
       Density[i] ~ dlnorm( log.median.data[i], tau)
    }

    # tau is 1/(sd.log * sd.log)
    tau <- 1/(sd.log*sd.log)
    sd.log ~ dunif(.05, 3)   # on the log-scale sd is proportion of the mean
    
    # priors for the intercept and slope
    beta0 ~ dnorm(0, .001)
    beta1 ~ dnorm(0, .001)

    # random effect of Year 
    for(i in 1:Nyears){
          year.eff[i] ~ dnorm(0, tau.year.eff)
    }
    tau.year.eff <- 1/(sd.year.eff*sd.year.eff)
    sd.year.eff  ~ dunif(.01,2)

    # random effect of Sites
    for(i in 1:Nsites){
          site.eff[i] ~ dnorm(0, tau.site.eff)
    }
    tau.site.eff <- 1/(sd.site.eff * sd.site.eff)
    sd.site.eff  ~ dunif(.01, 2)

    # what is the probability that the trend is negative
    p.beta1.lt.0 <- ifelse(beta1<0,1,0)

   # derived variables.
   for(i in 1:Nyears){
      med.den.trend [i] <- exp(log.median.trend  [i])
      med.den.data  [i] <- exp(log.median.data   [i])
   }

   # probability of being in a threshold category for trend line
   for(i in 1:Nyears){
      for(k in 1:NFSI){
          prob.FSI.cat.trend[i,k] <- ifelse((med.den.trend[i] >= FSI.lower[k]) && 
                                            (med.den.trend[i] <  FSI.upper[k]),1,0)
          prob.FSI.cat.data [i,k] <- ifelse((med.den.data [i] >= FSI.lower[k]) && 
                                            (med.den.data [i] <  FSI.upper[k]),1,0)
      }
   }
}
") # End of the model
```

We need to recode the calendar years into values from 1 (for the earliest year)
to xx (for the maximum year) in the data. Because JAGS cannot process
character data, we also need to create a numeric code for the sites.
The `data.list` has the coded year and site numbers and the `site.code` and
`year.code` data frames have the translation between the site and year codes
and the actual site and year values.

We will the trend model to the CTTR species data.

```{r fit.multiyear, echo=TRUE}
# Create the data list

pass.cttr <- pass.melt[ !is.na(pass.melt$Density) & pass.melt$Species=="CTTR",]

year.code <- data.frame(Year    =sort(unique(pass.cttr$Year)), 
                        Year.num=1:length(unique(pass.cttr$Year)))
year.code
pass.cttr <- merge(pass.cttr, year.code)
head(pass.cttr)

# Convert Site.no to a unique numeric values
site.code <- data.frame(Site.No  =unique(pass.cttr$Site.No),
                        Site.num =1:length(unique(pass.cttr$Site.No)), 
                        stringsAsFactors=FALSE)
site.code
pass.cttr <- merge(pass.cttr, site.code)


pass.cttr <- pass.cttr[ order(pass.cttr$Year.num, pass.cttr$Site.num),]


data.list <- list(Ndata      =nrow(pass.cttr),
                  Year.num   =pass.cttr$Year.num,
                  Year       =pass.cttr$Year,
                  Site.num   =pass.cttr$Site.num,
                  Density =pass.cttr$Density+.1*min(pass.cttr$Density[pass.cttr$Density>0]),
                  NFSI       =nrow(FSI.threshold),
                  FSI.lower  =FSI.threshold$lower,
                  FSI.upper  =FSI.threshold$upper)
data.list

# Next create the initial values.
# If you are using more than one chain, you need to create a function
# that returns initial values for each chain.

init.list <- list(
   list(), list(), list()
)

# Next create the list of parameters to monitor.
# The deviance is automatically monitored.
# 
monitor.list <- c("log.median.data","log.median.trend",
                  "med.den.data","med.den.trend",
                  "site.eff","year.eff",
                  "sd.log","sd.year.eff","sd.site.eff",
                  "prob.FSI.cat.trend","prob.FSI.cat.data",
                  "beta0","beta1","p.beta1.lt.0")

```

We again fit the model in JAGS using `R2JAGS` and save
the MCMC output in the `result.cttr` list.

```{r yearly.fit  }
set.seed(4532234)  # intitalize seed for MCMC 

results.cttr <- jags( 
  data      =data.list,   # list of data variables
  inits     =init.list,   # list/function for initial values
  parameters=monitor.list,# list of parameters to monitor
  model.file="model2.txt",  # file with bugs model
  n.chains=3,
  n.iter  =5000,          # total iterations INCLUDING burn in
  n.burnin=2000,          # number of burning iterations
  n.thin=2,               # how much to thin
  DIC=TRUE,               # is DIC to be computed?
  working.dir=getwd()    # store results in current working directory
)

```




In exactly the same way, a sample from the posterior of the estimated median based on the underlying trend can be found using MCMC methods. Each value from the posterior sample
from the median can be compared to the FSI categories and the probabilistic assessment
can be computed based on the underlying trend. This should be more stable because the
year-specific effect have been "removed" before the probabilistic assessment is made.


The estimated slope, a measure of uncertainty of the estimate, and
the posterior belief that the slope is negative can be extracted
from the MCMC output:
```{r beta.table, echo=FALSE}
beta.table <- data.frame(slope      =results.cttr$BUGSoutput$mean$beta1,
                         sd         =results.cttr$BUGSoutput$sd$beta1,
                         p.slope.lt.0=results.cttr$BUGSoutput$mean$p.beta1.lt.0, 
                         stringsAsFactors=FALSE)

beta.table
```

The estimated slope of the trend line for the log(median) is
`r round(beta.table$slope,3)` (SE `r round(beta.table$sd,3)`) 
and the posterior belief that the slope is POSITIVE is `r round(1-beta.table$p.slope.lt.0,3)`.

```{r echo=FALSE}
year.post.plot.fignum <- getFIGNUM()
```
We can create plots of the posterior density of the median density for each year
in the study as shown in Figure `r year.post.plot.fignum`.

```{r year.postplot, echo=FALSE, message=FALSE, warning=FALSE, fig.height=3, fig.width=5, dpi=300}
# Extract the posterior mean for the estimated median (including process error)
select <- grepl('^med.den.data[', row.names(results.cttr$BUGSoutput$summary), fixed=TRUE)
meddata <- data.frame(med.density=matrix(results.cttr$BUGSoutput$mean$med.den.data, ncol=1))
meddata$Year.num <- 1:nrow(results.cttr$BUGSoutput$mean$med.den.data)
meddata <- merge(meddata, year.code)
#head(meddata)

# Extract the underlying trend
select <- grepl('^med.den.trend', row.names(results.cttr$BUGSoutput$summary))
trenddata <- data.frame(med.density=results.cttr$BUGSoutput$summary[select, "mean"])
# convert the year,species code to actual years and species
trenddata$Year.num   <- 1:nrow(trenddata)
trenddata <- merge(trenddata, year.code)
#head(trenddata)

# get the posterior density values
select <- grepl('^med.den.trend', colnames(results.cttr$BUGSoutput$sims.matrix))
plotdata <- reshape2::melt(as.data.frame( results.cttr$BUGSoutput$sims.matrix[, select]),
                           variable.name='Year.num',
                           value.name='med.density')
plotdata$Year.num <- gsub("med.den.trend[","", plotdata$Year.num, fixed=TRUE)
plotdata$Year.num <- as.numeric(gsub("]",  "", plotdata$Year.num, fixed=TRUE))

plotdata <- merge(plotdata, year.code)

# make a plot of the posterior median for each year with the superimposed trend

postplot <- ggplot2::ggplot( data=meddata, aes(x=Year, y=med.density))+
  ggtitle(paste("Figure ", year.post.plot.fignum,". Estimated MEDIAN density with trend line",sep=""))+
  ylab("Median from fitted model and posterior beliefs")+
  geom_point(data=plotdata, aes(group=Year), alpha=0.01, position=position_jitter(w=0.2))+
  geom_point()+
  geom_line(color="black", size=1)+
  geom_line(data=trenddata, aes(x=Year, y=med.density), color="blue", size=2)+
  geom_hline(data=FSI.threshold[1:3,], aes(yintercept=lower), alpha=1, color="red")+
  ylim(0,150)
postplot

```
The blue line is the estimated trend in the median while the black line
is the estimated median in each year adjusting for the sites that were
sampled and potential process error. In this case, there were 2 years around 2013
where the yearly median declined, but this also happened aroung 2005
so may not be unusual.


```{r echo=FALSE}
year.fsi.fignum <- getFIGNUM()
```
Similarly, we can estimate the probability of belonging to each
FSI category over time as shown in Figure `r year.fsi.fignum`.

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.height=3, fig.width=5, dpi=300}
# plot of the probability of being in each category over time for the underlying trend
prob.FSI.cat <- results.cttr$BUGSoutput$mean$prob.FSI.cat.trend

plotdata <- data.frame(prob=matrix(prob.FSI.cat,ncol=1))
plotdata$Year.num    <- rep(1:nrow(year.code))
plotdata$FSI.num.cat <- rep(1:nrow(FSI.threshold), each=nrow(year.code))

# convert the year, species, and FSI category numeric code to actual year, species, FSI
plotdata <- merge(plotdata, year.code,        all=TRUE)
plotdata <- merge(plotdata, FSI.threshold[,c("FSI.num.cat","FSI.cat")])
#head(plotdata)


# for stacked bar charts for each species
# we need to sort the data frame in reverse order of FSI category
plotdata <- plotdata[ order(plotdata$FSI.cat),] #, decreasing=TRUE),]
plotdata$FSI.cat2 <- factor(plotdata$FSI.cat, levels=rev(levels(plotdata$FSI.cat)), order=TRUE)
fsi.plot <- ggplot(data=plotdata, aes(x=Year, y=prob, fill=FSI.cat2))+
  ggtitle(paste("Figure ", year.fsi.fignum,". Probability of being in FSI category",
                "\nbased on underlying trend line",sep=""))+
  ylab("Cumulative probability of being in FSI category")+
  geom_bar(stat="identity")+
  scale_fill_brewer(palette="RdYlGn", direction =-1, name="FSI\nCategory")
fsi.plot

```

We see with the estimated increasing trend in the median, that the
probability has declined in the VHR fish category.

```{r echo=FALSE}
fsi.data.fignum <- getFIGNUM()
```

THe impact of process error (year-specific effects) can be 
seen by looking at the FSI categorization without the
smoothing trend applied as shown in Figure `r fsi.data.fignum`.

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.height=3, fig.width=5, dpi=300}
# plot of the probability of being in each category over time 
# allowing for process error 
prob.FSI.cat <- results.cttr$BUGSoutput$mean$prob.FSI.cat.data

plotdata <- data.frame(prob=matrix(prob.FSI.cat,ncol=1))
plotdata$Year.num    <- rep(1:nrow(year.code))
plotdata$FSI.num.cat <- rep(1:nrow(FSI.threshold), each=nrow(year.code))

# convert the year, species, and FSI category numeric code to actual year, species, FSI
plotdata <- merge(plotdata, year.code,        all=TRUE)
plotdata <- merge(plotdata, FSI.threshold[,c("FSI.num.cat","FSI.cat")])
#head(plotdata)


# for stacked bar charts for each species
# we need to sort the data frame in reverse order of FSI category
plotdata <- plotdata[ order(plotdata$FSI.cat),] #, decreasing=TRUE),]
plotdata$FSI.cat2 <- factor(plotdata$FSI.cat, levels=rev(levels(plotdata$FSI.cat)), order=TRUE)
fsi.plot.data <- ggplot(data=plotdata, aes(x=Year, y=prob, fill=FSI.cat2))+
  ggtitle(paste("Figure ", fsi.data.fignum,". Probability of being in FSI category",
                "\n including year-specific effects", sep=""))+
  ylab("Cumulative probability of being in FSI category")+
  geom_bar(stat="identity")+
  scale_fill_brewer(palette="RdYlGn", direction =-1, name="FSI\nCategory")
fsi.plot.data

```

We can see from this figure, that year-specific effects (process
error) lead to much variability in the FSI categorization 
due to factors not under the control of the program.


In this study, the trend line is fit using the entire data series.
However, you may feel that this is too rigid and want some
flexibility. A cubic spline could be fit instead where
the fit of the spline for a particular year depends more
on the years surrounding the data point that on years that
are very far away (in time) from the particular year. This
has not yet been implemented.



# How much sampling is needed?
The question of sample size can be broken into two (separate) questions

1. How many years of sampling and how many sites must be sampled to detect a
trend? This is covered in a companion document.

2. How many sites must be sampled in a particular year
to be reasonably sure that the FSI
category is known with a high probability. 

This sections will consider the latter question.

The posterior probability that a site is classified into an FSI category depends on the value of the median 
and the standard deviation of the values for a particular year. More technically, it depends
on the $log(median)$ and the standard deviation of the $log(CPUE)$. For example,
if the median is close to the middle of a FSI category, then fewer samples
will be needed to determine the FSI category with high probability than if the
median is close to a boundary. Similary, if the standard deviation of the
CPUE is high, then more samples are needed to estimate the median with enough precision so that
most of the confidence interval lies within the FSI category
(i.e. the log(median) has a small standard error) than if the standard deviation is small.

There is no easy way to determine the required sample size in a year other than using the Bayesian model
with simulated data based on the median and standard deviation of the CPUE at various sample sizes. The Bayesian model used
is the same one as used for the analysis of the two streams presented earlier.

```{r avg.sd.log, echo=FALSE}
sd.log <- plyr::ddply(pass.cttr, "Year", summarize,
                      sd.log = sd(log(Density)))
avg.sd.log <- mean(sd.log$sd.log, na.rm=TRUE)

```

For example, using the Quirk Creek data shown earlier, the average 
standard deviation of the log(CPUE) for CTTR is `r round(avg.sd.log,2)`
which estimates the coefficient of variation 
of individual values around the median.

```{r sim.values, echo=FALSE}
med.list <- c(20, 30, 36, 50)
med.FSI.cat <- apply(outer(med.list, FSI.threshold$upper, "<="),1, which.max)

n.list <- c(5, 10, 200)
sim.fignum <- getFIGNUM()

```
This average standard deviation was used to estimate the probability of belonging
to the two worst FSI categories with different values of the median. The cutoffs
for the two worst FSI categories are 
`r paste(paste(FSI.threshold$lower[1:2],collapse=", "), ", and ", FSI.threshold$lower[3], sep="", collapse="")`. 
We used all combinations of values for the median of 
`r paste(paste(med.list[-length(med.list)],collapse=", "),' and ',med.list[length(med.list)], sep="", collapse="")`
and sample sizes of 
`r paste(paste(n.list[-length(n.list)],collapse=", "),' and ',n.list[length(n.list)], sep="", collapse="")`
to compute the posterior probability of belonging to the FSI categories as shown in Figure `r sim.fignum`.



```{r sim.scenarios, echo=FALSE, message=FALSE, warning=FALSE, include=FALSE}
scenarios <- expand.grid(median= med.list,
                         n     = n.list,
                         sd.log=avg.sd.log)
scenarios$scenario <- 1:nrow(scenarios)
res <- plyr::ddply(scenarios, "scenario", function (x){
   # generate simulated data and find the FSI categorization
   log.median <- log(x$median)
   gen.data   <- rlnorm(x$n, meanlog=log.median, sdlog=x$sd.log)
   
   data.list <- list(Ndata   =length(gen.data),
                  Density    =gen.data,  
                  NFSI       =nrow(FSI.threshold),
                  FSI.lower  =FSI.threshold$lower,
                  FSI.upper  =FSI.threshold$upper)
   data.list
   
   # Next create the initial values.

   init.list <- list(
      list(), list(), list()
   )

   # Next create the list of parameters to monitor to get posterior
   # 
   monitor.list <- c("log.median", "sd.log","median","prob.FSI.cat")
   
   # Finally, the actual call to JAGS
   set.seed(23234534)  # intitalize seed for MCMC 

   results <- jags( 
     data      =data.list,   # list of data variables
     inits     =init.list,   # list/function for initial values
     parameters=monitor.list,# list of parameters to monitor
     model.file="model1.txt",  # file with bugs model
     n.chains=3,
     n.iter  =5000,          # total iterations INCLUDING burn in
     n.burnin=2000,          # number of burning iterations
     n.thin=2,               # how much to thin
     DIC=TRUE,               # is DIC to be computed?
     working.dir=getwd()    # store results in current working directory 
   )
  
   prob.fsi.cat <- results$BUGSoutput$mean$prob.FSI.cat
   data.frame(x, FSI.num.cat=1:5, prob=prob.fsi.cat,
              FSI.cat=FSI.threshold$FSI.cat)
  
   })
   # sort the resutls by FSI category in each combination of median and n
   res <- res[ order(res$median, res$n, res$FSI.cat),] #, decreasing=TRUE),]
   res$FSI.cat2 <- factor(res$FSI.cat, levels=rev(levels(res$FSI.cat[1:5])), order=TRUE)
   res$n2 <- paste("n=",formatC(res$n,format="f", digits=0, width=2), sep="")
   res$median2 <- paste("med=",res$median,sep="")
   fsi.plot.sim<- ggplot(data=res, aes(x="FSI Category", y=prob, fill=FSI.cat2))+
     ggtitle(paste("Figure ", sim.fignum,". Probability of being in FSI category",
                "\n in one particular year from simulated data",
                "\n with cv around median of ", round(avg.sd.log,2), sep=""))+
     ylab("Cumulative probability of being in FSI category")+
     geom_bar(stat="identity")+
     scale_fill_brewer(palette="RdYlGn", direction =-1, name="FSI\nCategory")+
     facet_grid(median2~n2, scales="fixed")
```
```{r echo=FALSE}
   fsi.plot.sim
```

In this case, the median values are in FSI categories
`r paste(paste(med.FSI.cat[-length(med.FSI.cat)],collapse=", "),' and ',med.FSI.cat[length(med.FSI.cat)], sep="", collapse="")`
respectively. The plot shows that for median values that are far from the FSI category boundaries, relatively small
numbers of samples are needed to ensure that the correct categorization is assigned with high probability,
while for values of the median close to a boundard, larger samples sizes (number of sites) will be needed
to ensure that the correct categorization is assigned. 

This is not an unexpected result. In practise, the value of the median will not be known in advance
even if information about the coefficient of variation (the standard deviation of the log(CPUE)) is known
from past studies or from similar sites. It would seem prudent to plan sample sizes based
on the median being more than 5 units away from a category boundary for planning purposes.

CAUTION: The above analysis is the sample size within a year to categorize A PARTICULAR year into 
the FSI category. However, the results for a year are HEAVILY INFLUENCED by year-specific factors
(the process error) and so the assignment to an FSI category based on each year's results may 
result in high veriability in the assignments across years (as shown earlier) which makes
it difficult to assess long-term trends. For this reason, assessment of the TREND in
FSI categorization should be based on the trend line (which `removes' year-specific effects)
rather than inidividual yearly assessments. As noted in a companion document, 
the standard deviation of the year-specific effects (the process error) is often
the limiting factor to determining trend. In these cases, assessment of the FSI category 
for trend is relatively
insensitive to the number of sites sampled per year, and depends heavily on 
the total number of years of sampling rather than the amount of sampling in each year.






