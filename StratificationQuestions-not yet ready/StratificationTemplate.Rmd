---
title: "Your Favorite Watershed - Stratification Choices"
author: "Carl Schwarz"
date: '`r format(Sys.time(), "%Y-%m-%d")`'
output:
  html_document:
    number_sections: yes
    toc: yes
  pdf_document:
    number_sections: yes
    toc: yes
  word_document:
    fig_caption: yes
    reference_docx: WatershedTemplate-STYLE.docx
    toc: yes
---

```{r setup, include=FALSE}
# Links for RMarkdown help

# Information about RMarkdown is at: http://rmarkdown.rstudio.com

# Chunk options      https://yihui.name/knitr/options/

# Dealing with word templates and Rmarkdown
#    http://stackoverflow.com/questions/41982700/how-to-properly-number-headings-in-word-from-a-rmarkdown-document
#    http://rmarkdown.rstudio.com/articles_docx.html   
#    http://rmarkdown.rstudio.com/word_document_format.html

# Information on using ggmap is available at
#    https://github.com/dkahle/ggmap/blob/master/README.md
# You may need to intall the latest version (not available on CRAN) if you get the message about ggproto. 
# Use 
#    devtools::install_github("dkahle/ggmap")

#    http://github.com/zoul/Finch/zipball/master/

# Here we open the libraries needed and source any other code as needed
knitr::opts_chunk$set(echo = TRUE)

Version <- '2071-05-05'

library(car)
library(coda)
library(ggmap)
library(ggplot2)
library(gridExtra)
library(knitr)
library(lmerTest)
library(pander)
library(plyr)
library(readxl)
library(reshape2)
library(scales)
library(stringr)

source("functions.R")  # functions to read the FWIS workbook etc

# Set Figure and Table number variables. DO NOT CHANGE these except at your peril
# Get the figure or table number before the first time you reference it.
.FIGNUM <- 0
.TABNUM <- 0


# Get the raw data and do editing. This won't appear in the report, but gives you 
# an audit trail of what you did to the raw data before creating the rest of the
# template

target.species <- 'ALL'
#target.species <- c("BKTR","BLTR","MNWH") # or you can select which species to include

# Enter the workbook name and worksheet name containing the FWIS output

workbookName = file.path('Data','Wampus Creek.xls')
sheetName    = 'F&W'
csvfilename  = file.path("Data","Wampus Creek.csv")

workbookName = file.path('Data','Freeman HUC8.xls')
sheetName    = 'F&W 2008 & 2015 '
csvfilename  = file.path("Data", "Freeman HUC8.csv")

workbookName = file.path('Data','Quirk Creek.xls')
sheetName    = 'F&W'
csvfilename  = file.path("Data","Quirk Creek.csv")


survey.type.wanted <- tolower(c('Electrofishing'))  # make sure spelling is correct!

# CHoose one of the following code fragments, depending if JAVA is working or not.
#  If xlxs package is working
# Check that  FWIS files exists
if(!file.exists(workbookName))stop(paste("File ",workbookName,' not found '))

# read the sheet from the workbook
fish <- read.FWIS.workbook(
        workbookName  =workbookName,
        sheetName     =sheetName,
        target.species=target.species)

# Otherwise read using csv
# Check that  FWIS files exists
#if(!file.exists(csvfilename))stop(paste("File ",csvfile,' not found '))
#
#fish <- read.FWIS.workbook.csv(
#        csvfilename,
#        target.species=target.species)

xtabs(~Survey.Type + Year,     data=fish , exclude=NULL, na.action=na.pass)
# Remove all non-electrofishing survey types
dim(fish)
fish <- fish[ tolower(fish$Survey.Type) %in% survey.type.wanted,]
dim(fish)
xtabs(~Survey.Type + Year,     data=fish , exclude=NULL, na.action=na.pass)

xtabs(~Species.Code, data=fish, exclude=NULL, na.action=na.pass)


# we need to create 3 files (corresponding to 3rd normal forms for relational tables)
# for further processing
#  (a) Inventory records - when and where was an inventory taken
#          watershedName Year Location Inventory.Survey.ID
# 
#  (b) Catch Summary - how many fish by species were captured. 
#      We expand (a) to include catch including 0 catch for all species seen during any inventory
#          WatershedName Year Location Inventory.Survey.Id  Distance Time Species.Code Catch
# 
#  Cc) Fish length records - records for fish actually captured and measured this is basically the FWIS data readin
#          WatershedName Year Location Inventory.Survey.ID  Species.Code Fork.Length..m. 

#---------------------------------------
# Get the inventory records 
inventory <- plyr::ddply(fish, c("Inventory.Survey.ID"), function(x){
    # just pull out the first record and drop all of the fish information
    inventory <- x[1,, drop=FALSE]
    drop.columns <- c("Angler.Count","Pass.Number","Captured.Count","Total.Count.of.Species.by.SurveyID",
                      "Species.Code","Species.Common.Name","Weight..g.","Fork.Length..mm.",
                      "Total.Length..mm.","Gender","Age")
    inventory <- inventory[, !names(inventory) %in% drop.columns]
    as.data.frame(inventory)
})

xtabs(~Survey.Type + Year,     data=inventory , exclude=NULL, na.action=na.pass)
# Remove all non-electrofishing survey types
dim(inventory)
inventory <- inventory[ tolower(inventory$Survey.Type) %in% survey.type.wanted,]
dim(inventory)
xtabs(~Survey.Type + Year,     data=inventory , exclude=NULL, na.action=na.pass)

# Check the inventory records to make sure that everything is consistent
# 
inventory[,c("WatershedName","Year","Activity.Date","Inventory.Survey.ID")]

# what years are present in this workbooks?
xtabs(~Year, data=inventory, exclude=NULL, na.action=na.pass)

xtabs(~LocationTTM+Year,      data=inventory , exclude=NULL, na.action=na.pass)
# are there any dates that are invalid
inventory[ is.na(inventory$Year),]

# check the Distance and time fields
xtabs(~Distance..m., data=inventory, exclude=NULL, na.action=na.pass)
xtabs(~LocationTTM+Distance..m., data=inventory, exclude=NULL, na.action=na.pass)

# impute a distance field
inventory$Distance..m.[ is.na(inventory$Distance..m.)] <- 100
xtabs(~Distance..m., data=inventory, exclude=NULL, na.action=na.pass)
xtabs(~LocationTTM+Distance..m., data=inventory, exclude=NULL, na.action=na.pass)

#xtabs(~Inventory.Survey.ID+Distance..m., data=inventory, exclude=NULL, na.action=na.pass)

xtabs(~Time..s.,     data=inventory, exclude=NULL, na.action=na.pass)# remove data where no distance or time


#---------------------------------------------------------------------------------------
#  Create the catch summary
# We first extract for each Inventory.Survey.ID, one record per species
catch.summary <- plyr::ddply(fish, c("Species.Code","Inventory.Survey.ID"), plyr::summarize,
                            Count        = mean(Total.Count.of.Species.by.SurveyID))
# Any inventory with 0 counts for all species will have Species.Code = NA
catch.summary[ is.na(catch.summary$Species.Code),]
dim(catch.summary)
catch.summary <- catch.summary[ !is.na(catch.summary$Species.Code),]
dim(catch.summary)

# Now we need to expand for each Inventory.Survey.ID the full set of species 
# and impute 0 values for species not seen for this inventory (i.e 0 catch of this species)
which.species <- unique(catch.summary$Species.Code)
all.catch <- plyr::ddply(inventory, "Inventory.Survey.ID", function(x, species.list){
     all.species <- expand.grid(Inventory.Survey.ID=x$Inventory.Survey.ID[1],
                                Species.Code       =species.list,
                                stringsAsFactors=FALSE)
     all.species
}, species.list=which.species)
catch.summary <- merge(catch.summary, all.catch, all=TRUE)
catch.summary$Count[ is.na(catch.summary$Count)] <- 0
catch.summary <- merge(catch.summary, inventory, all.x=TRUE)   # merge it back with inventory records get inventory infor

catch.summary <- plyr::ddply(catch.summary, c("WatershedName","Species.Code","Year","LocationTTM"), plyr::summarize,
                             Distance..m. = sum(Distance..m., na.rm=TRUE),
                             Count        = sum(Count,        na.rm=TRUE),
                             CPUE.300m    = Count / Distance..m. * 300)

xtabs(~Species.Code+Year, data=catch.summary, exclude=NULL, na.action=na.pass)
xtabs(Count~Species.Code+Year, data=catch.summary, exclude=NULL, na.action=na.pass)
xtabs(~LocationTTM+Year, data=catch.summary, exclude=NULL, na.action=na.pass)
xtabs(~LocationTTM+Species.Code+Year, data=catch.summary[catch.summary$Year==2015,], exclude=NULL, na.action=na.pass)

```


```{r eval=FALSE, echo=FALSE}
# Refer to the cheat sheets for RMarkdown.

# Common problems include:
# - Don't add extra spaces after a periods before a new line. Otherwise the
#   text will break at this point and not run together.
```

# Background
The assignment of a watershed to the FSI category depends on the median CPUE estimated
for that watershed and the uncertainty in the estimated median. The uncertainty
in the estimated median in turn depends upon the variability in the CPUE values and
the number of CPUE values measured in a particular year.

In the case of a simple random sample, the uncertainty in the median can be easily
computed. Let $Y$ represent the $log(CPUE)$. Then the $log(median)$ of the CPUE is estimated
as the simple sample mean of the $log(CPUE)$, i.e. $\overline{Y}$. The uncertainty
of $\overline{Y}$ is denoted as the standard error (SE) and is computed as $SE(\overline{Y})=s/\sqrt(n)$
where $s$ is the sample standard deviation of $Y$ and $n$ is the sample size.

In some cases, the uncertainty in the $\log(median)$ CPUE can be reduced by stratifying
the CPUE. Two potential stratification variables are the stream order or the lower levels
of the HUC classification (e.g. the watershed or sub-watershed levels).

In order to compute a mean from a stratified design, on additonal piece of information is
needed, namely the relative (population) weights of the strata. For example, the entire sub-basin
could composed of 30, 20, and 10 streams of order 2, 3 and 4 respectively. Then the weights for the stream-order
stratificaton are $30/60=0.5$; $20/50=0.333$; and $10/60=0.167$ respectively. Notice that the population
weights do not have to correspond to the division by sample size, i.e., you could over/under sample
in a stratum relative to its population weight.

After stratification, let

* $n_i$ be the observed sample size in stratum $i$, $i=1,...H$ where $H$ is the number of strata.

* $\overline{Y}_i$ be the sample mean for stratum $i$,

* $s_i$ be the sample standard deviation for stratum $i$, 

* $SE_i$ be the standard error of the mean in stratum $i$ computed as $SE_i =i/\sqrt(n_i)$, and

* $W_i$ be the population weight for stratum $i$.

The overall mean is found as a weighted average of the stratum means:
$$\overline{Y}_{overall} = W_1 \overline{Y}_1 + W_2 \overline{Y}_2 + ...$$
The standard error of the overall mean is found as
$$SE(\overline{Y}_{overall}) = \sqrt{W_1^2 SE_1^2 + W_2^2 SE_2^2 + ... }$$
A stratification could lead to reductions in the uncertainty of the overall mean if the values within a stratum
have a smaller standard deviation than values across strata, i.e. units within a strataum are homogeneous while
units across strata are heterogeneous.

How should units be allocated to strata? There are several possible methods, but the most common are:

* Equal allocation where the total sample size is divided equally among the strata.

* Proportional allocation where the total sample size is divided proportionally among the strata in
the same proportion as the population weights. For example, if one stratum had a population weight ($W_i$)
of 0.4, the 40% of the samples should be allocated to this stratum. In other words, larger strata
receive more samples.

* Optimal allocation where the total sample size is divided proportionally to the PRODUCT of the population
weights and the population standard deviations, i.e .proportion to $W_i S_i$ where $S_i$ is the population
standard deviation. Normally, the population standard deviation is unknown, but can be estimated from
the respective stratum sample standard deviation ($s_i$). In other words, large and more variable 
strata receive more samples.

In most cases, moving from an equal allocation to a proportional allocation provides a large improvement
in precision while moving to the optimal allocation only provides a further (modest) improvement.

It is straight forward to compare the efficacy of alternate stratification methods by finding 
the characteristics of the stratum (mean, standard deviation), dividing a proposed sample by the
various allocation methods (equal, proportional or optimal) and then comparing the resulting standard
errors of the mean.






